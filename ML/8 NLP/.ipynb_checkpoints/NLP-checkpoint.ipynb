{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Reviews.tsv', delimiter = '\\t', quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer #For reducing the dimension of the sparse matrix. Love vs loved\n",
    "corpus = []\n",
    "for i in range(0, 1000):\n",
    "    review = dataset['Review'][i]\n",
    "    # Cleaning Step 1 : re.sub -> Replaces one thing with other. All things like ,'' etc with space\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    # Cleaning Step 1 continued -> Replaces words like don't can't with do not, can not\n",
    "    review = re.sub(r\"\\b([a-zA-Z]{1,})n't\\b\", r'\\1 not', review)\n",
    "    # Cleaning Step 2 : Converting all to lower case.\n",
    "    review = review.lower() \n",
    "    # Cleaning Step 3 : Splitting review into single words that can be used in stemming later.\n",
    "    review = review.split() \n",
    "    ps = PorterStemmer()\n",
    "    # Defining all english stepwords\n",
    "    all_stopwords = stopwords.words('english') \n",
    "    # To not include the not in the stopwords, as not gives essential info about the data\n",
    "    all_stopwords.remove('not')\n",
    "    # Cleaning Step 4 : Getting rid of stop words, by applying stemming\n",
    "    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]    \n",
    "    # Joining all words together back to form the review, including space intentionally.\n",
    "    review = ' '.join(review)\n",
    "    # Appending back to corpus\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(corpus)\n",
    "#for x in corpus:\n",
    "#    print(\"--> \", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "# Fit will take all the words, Transform will put those in the columns\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "#print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55 42]\n",
      " [12 91]]\n",
      "Accuracy =  73.00\n",
      "Precision =  68.42\n",
      "F1 Score =  77.12\n",
      "Recall Score =  88.35\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(\"Accuracy = \",\"{:.2f}\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision = \",\"{:.2f}\".format(precision_score(y_test, y_pred)*100))\n",
    "print(\"F1 Score = \",\"{:.2f}\".format(f1_score(y_test, y_pred)*100))\n",
    "print(\"Recall Score = \",\"{:.2f}\".format(recall_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear', random_state=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[79 18]\n",
      " [24 79]]\n",
      "Accuracy =  79.00\n",
      "Precision =  81.44\n",
      "F1 Score =  79.00\n",
      "Recall Score =  76.70\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(\"Accuracy = \",\"{:.2f}\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision = \",\"{:.2f}\".format(precision_score(y_test, y_pred)*100))\n",
    "print(\"F1 Score = \",\"{:.2f}\".format(f1_score(y_test, y_pred)*100))\n",
    "print(\"Recall Score = \",\"{:.2f}\".format(recall_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Random forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[87 10]\n",
      " [45 58]]\n",
      "Accuracy =  72.50\n",
      "Precision =  85.29\n",
      "F1 Score =  67.84\n",
      "Recall Score =  56.31\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(\"Accuracy = \",\"{:.2f}\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision = \",\"{:.2f}\".format(precision_score(y_test, y_pred)*100))\n",
    "print(\"F1 Score = \",\"{:.2f}\".format(f1_score(y_test, y_pred)*100))\n",
    "print(\"Recall Score = \",\"{:.2f}\".format(recall_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[74 23]\n",
      " [45 58]]\n",
      "Accuracy =  66.00\n",
      "Precision =  71.60\n",
      "F1 Score =  63.04\n",
      "Recall Score =  56.31\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(\"Accuracy = \",\"{:.2f}\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision = \",\"{:.2f}\".format(precision_score(y_test, y_pred)*100))\n",
    "print(\"F1 Score = \",\"{:.2f}\".format(f1_score(y_test, y_pred)*100))\n",
    "print(\"Recall Score = \",\"{:.2f}\".format(recall_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[89  8]\n",
      " [36 67]]\n",
      "Accuracy =  78.00\n",
      "Precision =  89.33\n",
      "F1 Score =  75.28\n",
      "Recall Score =  65.05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(\"Accuracy = \",\"{:.2f}\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision = \",\"{:.2f}\".format(precision_score(y_test, y_pred)*100))\n",
    "print(\"F1 Score = \",\"{:.2f}\".format(f1_score(y_test, y_pred)*100))\n",
    "print(\"Recall Score = \",\"{:.2f}\".format(recall_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[80 17]\n",
      " [28 75]]\n",
      "Accuracy =  77.50\n",
      "Precision =  81.52\n",
      "F1 Score =  76.92\n",
      "Recall Score =  72.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(\"Accuracy = \",\"{:.2f}\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision = \",\"{:.2f}\".format(precision_score(y_test, y_pred)*100))\n",
    "print(\"F1 Score = \",\"{:.2f}\".format(f1_score(y_test, y_pred)*100))\n",
    "print(\"Recall Score = \",\"{:.2f}\".format(recall_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[78 19]\n",
      " [31 72]]\n",
      "Accuracy =  75.00\n",
      "Precision =  79.12\n",
      "F1 Score =  74.23\n",
      "Recall Score =  69.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(\"Accuracy = \",\"{:.2f}\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision = \",\"{:.2f}\".format(precision_score(y_test, y_pred)*100))\n",
    "print(\"F1 Score = \",\"{:.2f}\".format(f1_score(y_test, y_pred)*100))\n",
    "print(\"Recall Score = \",\"{:.2f}\".format(recall_score(y_test, y_pred)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
